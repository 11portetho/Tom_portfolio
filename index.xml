<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Thomas Porter</title>
    <link>https://11portetho.github.io/Tom_portfolio/</link>
    <description>Recent content on Thomas Porter</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 Sep 2022 10:58:08 -0400</lastBuildDate><atom:link href="https://11portetho.github.io/Tom_portfolio/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Topic modelling of BBC news articles using Latent Dirichlet allocation</title>
      <link>https://11portetho.github.io/Tom_portfolio/post/project-2/</link>
      <pubDate>Thu, 08 Sep 2022 10:58:08 -0400</pubDate>
      
      <guid>https://11portetho.github.io/Tom_portfolio/post/project-2/</guid>
      <description>This project provides an introduction to the latent Dirichlet allocation (LDA) model - a commonly used generative topic model which allocates a set of topics to a collection of documents by assuming each document exhibits each topic to different extents and each topic exhibits each word in the corpus to different extents. We discuss stochastic variational inference as a method of posterior inference for LDA. This combines stochastic optimisation and variational inference in order to approximate the posterior distribution of the latent variables given the observations in the LDA model.</description>
    </item>
    
    <item>
      <title>Cluster analysis of handwritten digits</title>
      <link>https://11portetho.github.io/Tom_portfolio/post/project-1/</link>
      <pubDate>Wed, 05 May 2021 10:58:08 -0400</pubDate>
      
      <guid>https://11portetho.github.io/Tom_portfolio/post/project-1/</guid>
      <description>This project provides an introduction to cluster analysis along with an example of its application on the MNIST dataset: a collection of 70,000 images of handwritten digits from 0-9. We discuss three methods of clustering: K-means, OPTICS and the Gaussian mixture model (and Expectation Maximisation algorithm) before describing principal component analysis, a common method used for data compression. These methods are subsequently applied to the MNIST dataset with their efficacy being evaluated using a number of both internal and external methods.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://11portetho.github.io/Tom_portfolio/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://11portetho.github.io/Tom_portfolio/about/</guid>
      <description>A passion for problem solving and analytical thinking led me to undertake a BSc in Mathematics at Durham. Whilst there, I developed an interest in statistics which gave me an introduction to machine learning and data science. During the initial Covid lockdown, I decided to learn more about these subjects, so I started playing around with some machine learning models in Python. I was particularly intrigued by unsupervised learning; how it could be used to draw meaning from seemingly useless unlabeled datasets.</description>
    </item>
    
    <item>
      <title>CV</title>
      <link>https://11portetho.github.io/Tom_portfolio/cv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://11portetho.github.io/Tom_portfolio/cv/</guid>
      <description>My CV can be found here:</description>
    </item>
    
  </channel>
</rss>
